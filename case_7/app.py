import streamlit as st
import pandas as pd
import plotly.express as px
from huggingface_hub import hf_hub_download
from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
import base64

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Amazon Reviews Dashboard",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üõí Amazon US Product Reviews Dashboard")
st.markdown("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ Hugging Face (3‚ÄØ–ì–ë) —Å –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ —á—Ç–µ–Ω–∏–µ–º –±–∞—Ç—á–∞–º–∏.")

# 1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å Hugging Face —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
@st.cache_data(show_spinner=True)
def get_csv_file():
    token = st.secrets["HF_TOKEN"]
    path = hf_hub_download(
        repo_id="PbI4a/Case_7",
        filename="clean_reviews.csv",
        repo_type="dataset",
        use_auth_token=token
    )
    return path

file_path = get_csv_file()

# 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≥–æ–¥–æ–≤ –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ (–ø–µ—Ä–≤—ã–π —Å—ç–º–ø–ª)
@st.cache_data(show_spinner=True)
def sample_for_filters(file_path):
    sample = pd.read_csv(file_path, nrows=200_000, parse_dates=['review_date'])
    years = sorted(sample['review_date'].dt.year.dropna().unique())
    products = sample['product_title'].dropna().unique()
    return years, products

years, all_products = sample_for_filters(file_path)

# 3. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Ñ–∏–ª—å—Ç—Ä–æ–≤
st.sidebar.header("üîç –§–∏–ª—å—Ç—Ä—ã")

yr_min, yr_max = st.sidebar.select_slider(
    "–ì–æ–¥ –æ—Ç–∑—ã–≤–∞",
    options=years,
    value=(years[0], years[-1])
)

ratings = st.sidebar.multiselect(
    "–†–µ–π—Ç–∏–Ω–≥ (–∑–≤—ë–∑–¥—ã)",
    [1, 2, 3, 4, 5],
    default=[1, 2, 3, 4, 5]
)

products = st.sidebar.multiselect(
    "–ü—Ä–æ–¥—É–∫—Ç—ã (–¥–æ 5)",
    all_products,
    max_selections=5,
    default=list(all_products[:3])
)

verified = st.sidebar.checkbox("–¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏", value=False)

filters = {
    'year_min': yr_min,
    'year_max': yr_max,
    'ratings': ratings,
    'products': products if products else None,
    'verified': verified
}

# 4. –§—É–Ω–∫—Ü–∏—è —á—Ç–µ–Ω–∏—è CSV –±–∞—Ç—á–∞–º–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
def read_filtered_chunks(file_path, filters, chunksize=100_000):
    for chunk in pd.read_csv(file_path, chunksize=chunksize, parse_dates=['review_date']):
        chunk['year'] = chunk['review_date'].dt.year
        chunk['sentiment_simple'] = chunk['star_rating'].apply(
            lambda r: 'negative' if r <= 2 else 'neutral' if r == 3 else 'positive'
        )
        chunk['polarity'] = chunk['review_body'].fillna("").map(lambda t: TextBlob(t).sentiment.polarity)

        mask = (
            (chunk['year'] >= filters['year_min']) &
            (chunk['year'] <= filters['year_max']) &
            (chunk['star_rating'].isin(filters['ratings']))
        )
        if filters.get('products'):
            mask &= chunk['product_title'].isin(filters['products'])
        if filters.get('verified'):
            mask &= chunk['verified_purchase'] == 'Y'

        filtered = chunk.loc[mask]
        if not filtered.empty:
            yield filtered

# 5. –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
@st.cache_data(show_spinner=True)
def aggregate_filtered(file_path, filters):
    df_list = []
    for filtered_chunk in read_filtered_chunks(file_path, filters):
        df_list.append(filtered_chunk)
    if df_list:
        df = pd.concat(df_list, ignore_index=True)
    else:
        df = pd.DataFrame(columns=[
            'review_date', 'star_rating', 'review_body', 'product_title',
            'verified_purchase', 'sentiment_simple', 'polarity', 'year'
        ])
    return df

df_filtered = aggregate_filtered(file_path, filters)

# 6. –û–±–∑–æ—Ä (Overview)
st.header("üìä –û–±–∑–æ—Ä")

c1, c2, c3 = st.columns(3)
c1.metric("–û—Ç–∑—ã–≤—ã –≤—Å–µ–≥–æ", f"{len(df_filtered):,}")
c2.metric("–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥", round(df_filtered['star_rating'].mean(), 2) if not df_filtered.empty else "N/A")
c3.metric("–°—Ä–µ–¥–Ω—è—è –ø–æ–ª—è—Ä–Ω–æ—Å—Ç—å", round(df_filtered['polarity'].mean(), 2) if not df_filtered.empty else "N/A")

if not df_filtered.empty:
    fig = px.histogram(
        df_filtered,
        x='star_rating',
        nbins=5,
        title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤"
    )
    st.plotly_chart(fig, use_container_width=True)
else:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º.")

# 7. –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –º–µ—Å—è—Ü–∞–º
st.header("üìà –î–∏–Ω–∞–º–∏–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –º–µ—Å—è—Ü–∞–º")
if not df_filtered.empty:
    df_filtered['month'] = df_filtered['review_date'].dt.to_period('M').dt.to_timestamp()
    ts = df_filtered.groupby('month').size().reset_index(name='count')
    fig_ts = px.line(ts, x='month', y='count', title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –º–µ—Å—è—Ü–∞–º")
    st.plotly_chart(fig_ts, use_container_width=True)
else:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞.")

# 8. –û–±–ª–∞–∫–æ —Å–ª–æ–≤ (WordCloud)
st.header("‚òÅÔ∏è –û–±–ª–∞–∫–æ —Å–ª–æ–≤ –∏–∑ –æ—Ç–∑—ã–≤–æ–≤")

if not df_filtered.empty:
    text = " ".join(df_filtered['review_body'].dropna().astype(str).values)
    if text.strip():
        wc = WordCloud(width=800, height=400, background_color='white').generate(text)
        fig_wc, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis("off")
        st.pyplot(fig_wc)
    else:
        st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤.")
else:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤.")

# 9. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ –ø–æ TF-IDF –∏ KMeans (–Ω–∞ —Å—ç–º–ø–ª–µ)
st.header("üß© –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ (–Ω–∞ —Å—ç–º–ø–ª–µ)")

if len(df_filtered) > 1000:
    sample_size = 2000
else:
    sample_size = len(df_filtered)

if sample_size > 0:
    sample_reviews = df_filtered['review_body'].dropna().sample(sample_size, random_state=42).astype(str).tolist()

    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
    X = vectorizer.fit_transform(sample_reviews)

    n_clusters = 5
    km = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = km.fit_predict(X)

    df_clusters = pd.DataFrame({'review': sample_reviews, 'cluster': clusters})

    # –í—ã–≤–æ–¥–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    cluster_counts = df_clusters['cluster'].value_counts().sort_index()
    fig_cluster = px.bar(
        x=cluster_counts.index,
        y=cluster_counts.values,
        labels={'x': '–ö–ª–∞—Å—Ç–µ—Ä', 'y': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤'},
        title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º"
    )
    st.plotly_chart(fig_cluster, use_container_width=True)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –æ—Ç–∑—ã–≤–∞ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    for i in range(n_clusters):
        st.subheader(f"–ö–ª–∞—Å—Ç–µ—Ä {i}")
        for review_text in df_clusters[df_clusters['cluster'] == i]['review'].head(3):
            st.write(f"- {review_text[:300]}{'...' if len(review_text) > 300 else ''}")
else:
    st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")

# 10. –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ LDA (–Ω–∞ —Å—ç–º–ø–ª–µ)
st.header("üìö –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (LDA)")

if sample_size > 0:
    vectorizer_lda = TfidfVectorizer(stop_words='english', max_features=1000)
    X_lda = vectorizer_lda.fit_transform(sample_reviews)

    n_topics = 5
    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda.fit(X_lda)

    feature_names = vectorizer_lda.get_feature_names_out()

    for topic_idx, topic in enumerate(lda.components_):
        st.subheader(f"–¢–µ–º–∞ {topic_idx + 1}")
        top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]
        st.write(", ".join(top_words))
else:
    st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.")

# 11. –ü—Ä–æ—Å–º–æ—Ç—Ä –æ—Ç–∑—ã–≤–æ–≤ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
st.header("üìÑ –ü—Ä–æ—Å–º–æ—Ç—Ä –æ—Ç–∑—ã–≤–æ–≤")

page_size = st.number_input("–û—Ç–∑—ã–≤—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É", min_value=5, max_value=50, value=10, step=5)
total_pages = (len(df_filtered) - 1) // page_size + 1 if len(df_filtered) > 0 else 1
page = st.number_input("–°—Ç—Ä–∞–Ω–∏—Ü–∞", min_value=1, max_value=total_pages, value=1, step=1)
start = (page - 1) * page_size
subset = df_filtered.iloc[start:start + page_size][['review_date', 'product_title', 'star_rating', 'review_body']]

for _, row in subset.iterrows():
    with st.expander(f"{row['review_date'].date()} | ‚≠ê{row['star_rating']} | {row['product_title'][:30]}"):
        st.write(row['review_body'])


# 12. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
def convert_df_to_csv(df):
    return df.to_csv(index=False).encode('utf-8')

csv_data = convert_df_to_csv(df_filtered)

st.download_button(
    label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤—ã CSV",
    data=csv_data,
    file_name='filtered_reviews.csv',
    mime='text/csv'
)
